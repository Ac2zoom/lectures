MWL Lecture 3 Notes
ML Paradigms

1. Loss Functions 
    - People who work in machine learning are pessimists, so we use a loss 
        function to tell us how much our model is "losing". We'd like to 
        lose as little as possible.
    - L(y, ŷ) := the cost (loss) of predicting ŷ when the actual outcome is y.
        > ŷ := f(x) where f is our predictor and 
            x a labeled data point.
    - Least Squares (LS) Loss
        > L_LS(y, ŷ) := (y - f(x))^2
        > Also known as quadratic loss.
        > Is the squared distance between ŷ and y 
        > Common in regression.
    - 0-1 Loss
        > L_01(y, ŷ) := 1 if y ≠ ŷ, 0 else
    - Others
        > Hinge loss (classification), Euclidean (regression), ...
        > The loss function you use depends on properties of 
            the optimization problem.
        > Beyond the scope of this lecture
    - Empirical Loss
        > The average loss on some dataset D
    - Bayes Risk/Loss
        > This is the OPTIMAL loss on some dataset D,
            so it is a lower bound on the actual loss of any predictor
        > Usually not 0.
        > *A property of the dataset or true population distribution, 
            not of any given model class.*
        > See notes below if interested in exact definition.

2. Generative and Discriminative Classifiers
    - Two broad classes of classifiers (no pun intended), 
        > Each class has different properties.
    - Generative Classifiers
        > Assumes each class is generated by some distribution g_y.
        > We can derive a posterior probability distribution (i.e. P(Y=y|X)),
            for each class y if we know g_y and prior P(Y = y).
        > The method for finding g_y depends on the model, P(Y = y) depends on D.
        > Anthill analogy
    - Discriminative Classifiers
        > Does not care how the classes are generated, is only
            concerned with their boundaries.
        > The boundaries between classes are called "decision boundaries".    
    - Differences
        > Generative classifiers are proven to be *asymptotically* optimal 
            * That is, if we have infinite amounts of data, generative 
                classifiers will always perform at least as good 
                as discriminative classifiers.
        > But usually discriminative classifiers perform better when we have 
            a finite amount of data (i.e., in real life).
        > This is not the whole story,
            see ["On Discriminative vs. Generative Classifiers..." Ng, Andrew].
        > See slides below for more pros/cons

3. Likelihood
    - "Probability" and "Likelihood" have specific (though, related) meanings 
        in statistics.
    - When we talk about "probability", we ask: "What is the *probability* that a 
        coin would land on heads 6 times and tails 4 times given that it is fair?
    - When we talk about "likelihood", we ask: "What is the *likelihood* that this 
        coin is fair given that it landed on heads 6 times and tails 4 times?
    - So a probability is a measure of certainty about an *event*, given some 
        parameters, and a likelihood is a measure of certainty about a *parameter* 
        given some events.
    - In Stats/ML, we are given the "event" (the data points), and we'd like to 
        know how "likely" the parameters of our model are.
    - Remember, to find the probability of a sequence of *independent* events 
        occurring (like a coin toss), we multiply together the probability of 
        each individual event occurring.
        > So the probability of getting the sequence 1101001110 given 
            P(H) = P(T) = 0.5 is 0.5^10. If P(H) = 0.2 and P(T) = 1 - P(H) = 0.8,
            then the probability of such an event is considerably lower, 0.8^4*0.2^6.
    - Using Bayes Rule, we may find both conditional probabilities and/or 
        conditional likelihoods.
        > BUT, this is not usually directly applicable. If θ is our parameters
            and E our event, then Likelihood(θ) = L(θ) = P(θ|E) = P(θ)P(E|θ)/P(E).
        > What is P(θ)? P(E)? (Notice I didn't ask what is P(E|θ))
        > So instead we usually just define L(θ) := P(E|θ)
        > Of course, in statistical modeling, we'd like to maximize L(θ).
    - Maximizing L(θ)
        > To maximize L(θ), we must first be able to *calculate* L(θ)
        > Assuming our data points are independent, we can just assign each 
            data point some probability of occuring and multiply all those 
            probabilities together, right?
        > Yes... but NO. For a computer, 0.5^10000 = 0 < machine epsilon
        > This is called underflow.
        > So usually we consider instead the "log-likelihood", l(θ).
        > Instead of multiplying the probabilities together, we add together 
            the natural log of each probability.
        > Thus, l(θ) = ln(0.5^10000) = sum from i=1:10000 ln(0.5) =~ -6931
        > log has a number of nice mathematical properties that make it good
            for this purpose, chief among them being that it is a monotonically 
            increasing function.
            * So maximizing the log-likelihood, l(θ), is equivalent to maximizing 
                the likelihood, L(θ).
    - Finally, logistic regression!

4. Logistic Regression
    - Confusingly enough, logistic regression is used (by itself) as a classifier.
        > But it's also used as a building block in more complex models (like 
            neural networks).
        > Logistic regression is an instance of something called a Generalized
            Linear Model (GLM). I will not say anything on this except that 
            simple linear regression (another instance of a GLM), presupposes 
            that our response variables are normally distributed. But when 
            our response variables are obviously not normally distributed 
            (i.e., the response var is in {-1, +1}), we need a different 
            model. This is where logistic regression comes in.
    - Gives us a measure of how "certain" we are that a data point equals
        a case (i.e. y = +1).
    - We will consider the more common, binary case. Although there is also 
        what is called "multinomial" logistic regression.
    - Derivation
        > See the slides linked below. 
        > Wikipedia has a different approach.
    - Optimal model parameters must be found by numerical optimization.

5. Numerical Optimization
    - General idea: 
        > We have some loss function we'd like to minimize. 
        > This loss function is differentiable.
        > We may differentiate the loss function to find the gradient.
        > Picking some arbitrary point in "parameter space", we can 
            follow the gradient to a minima.
        > If we're smart about our loss function (i.e., it is convex), this 
            local minima will also be a global minima.
        > The global minima in "parameter space" are our optimal parameters
    - The parameters of logistic regression are most often optimized via 
        a technnique called 'maximum likelihood estimation'
        > MLE works exactly how it sounds. The "estimation" part is some sort 
            of numerical optimization method, like gradient descent.


Recommended Readings:

    - The first half of this lecture was heavily derived from STAT 535 
        Lecture 2 slides. Check them out for prettier equations and additional info:
    http://www.stat.washington.edu/courses/stat535/fall15/Handouts/l2slides-prediction-concepts.pdf
        
    - Logistic Regression Wiki 
    https://en.wikipedia.org/wiki/Logistic_regression

    Some paradigms we DIDN'T talk about:
        - Bias-Variance Tradeoff
        - Curse of Dimensionality
        - Parametric vs. Non-Parametric Models
        - No Free Lunch
