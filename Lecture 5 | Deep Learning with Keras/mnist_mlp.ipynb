{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning with MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a \"censored\" version of a Keras example \n",
    "from here: https://github.com/fchollet/keras\n",
    "\n",
    "This version is meant to be run on the CPU. \n",
    "It trains on less data and will give you less\n",
    "impressive predictive performance than the \n",
    "original example code (which is meant to be \n",
    "run on a GPU).\n",
    "\n",
    "Train a simple deep NN on the MNIST dataset.\n",
    "\n",
    "* 16 seconds per 6,000 data instance epoch on Intel Celeron N2840 @ 2.16GHz (Sigmoid)\n",
    "* 14 seconds per 6,000 data instance epoch on Intel Celeron N2480 @ 2.15GHz (ReLU)\n",
    "* 20 seconds per 6,000 data instance epoch on Intel Celeron N2480 @ 2.15GHz (ReLU, validation_data)\n",
    "* 2 seconds per 60,000 data instance epoch on a K520 GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ..., \n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ..., \n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ..., \n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       ..., \n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ..., \n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ..., \n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ..., \n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]]], dtype=uint8)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of our 60,000 handwritten digits comes prepackaged as a 28x28 matrix, which you can think of as a primitive 28x28 pixel photo of a digit. The elements in the matrix take on values between 0 and 255, depending on how dark the writing is in that \"pixel\". But we usually train our models on feature *vectors*, not matrices. Go ahead and use numpys *reshape* method to reshape our matrix into a 28x28 = 784-dimensional vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Signature\n",
    "----------\n",
    "np.reshape(a, newshape, order='C')\n",
    "\n",
    "Docstring\n",
    "----------\n",
    "Gives a new shape to an array without changing its data.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "a : array_like\n",
    "Array to be reshaped.\n",
    "\n",
    "newshape : int or tuple of ints\n",
    "    The new shape should be compatible with the original shape. If\n",
    "    an integer, then the result will be a 1-D array of that length.\n",
    "    One shape dimension can be -1. In this case, the value is inferred\n",
    "    from the length of the array and remaining dimensions.\n",
    "    \n",
    "Returns\n",
    "-------\n",
    "reshaped_array : ndarray\n",
    "    This will be a new view object if possible; otherwise, it will\n",
    "    be a copy.  Note there is no guarantee of the *memory layout* (C- or\n",
    "    Fortran- contiguous) of the returned array.\n",
    "\n",
    "See Also\n",
    "--------\n",
    "ndarray.reshape : Equivalent method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(60000, 784)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the same for our test set X_test. Don't forget to check how many data points it has with the *shape* method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test = X_test.reshape(10000, 784)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But since we're training on the CPU and don't want to wait all day for our models to train, we'll take the first 6000 data points for our actual training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = X_train[:6000]\n",
    "y_train = y_train[:6000]\n",
    "X_test = X_test[:5000] # we'll use a half-sized test set, too\n",
    "y_test = y_test[:5000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part just makes sure numpy casts our data values to the correct type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When training neural networks, it's important to normalize our data. There are multiple reasons for this. Normalizing lets us not worry about any scaling effects our input might have on our model (for example, one feature is measured in millimeters, and another in miles). Furthermore, when we randomly initialize the weights of our neurons, we won't have to worry (as much) that our neurons will become saturated. Saturation occurs when the output of our neuron is very near one or zero, no matter the input. Remember, the output of a (sigmoidal) neuron is a linear combination of its inputs passed into a logistic function. The gradient of the logistic function is very low when its output is near 1 or 0, hence the gradient in the backpropogation algorithm will be very small and our neurons weights will update too slowly.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize (Well, *standardize*, techinically) X_train so that its mean is 0 and standard deviatios 1. (Hint: subtract the mean from each datapoint then divide by the standard deviation). Notice we don't standardize the test data, because it isn't necessary. Our neural network will predict well even if we shift or stretch the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train -= np.mean(X_train)\n",
    "X_train /= np.std(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our output is a linear combination of some input from our hidden layer, we need to make sure our classes are one-hot encoded. For example, if our neural net is 50% sure a digit is a 2 and 50% sure it is a 4, we don't want our network predicting a 3!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class representation before one hot encoding:\n",
      "[5 0 4]\n",
      "Class representation after one hot encoding:\n",
      "[[ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Class representation before one hot encoding:\\n\" + str(y_train[0:3]))\n",
    "Y_train = np_utils.to_categorical(y_train, 10)\n",
    "Y_test = np_utils.to_categorical(y_test, 10)\n",
    "print(\"Class representation after one hot encoding:\\n\" + str(Y_train[0:3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can finally start building our model! *Sequential* is a typical neural network with layers stacked one on top of the other. There is another type of model, *Graph*, which is poorly named, because it can only be a directed acyclic graph (DAG). Can you imagine a neural network that violates one or both of the DAG properties? :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the blueprint for our model. It's your job to write the code. Reference the Keras docs (or look over [this quick introduction](https://github.com/fchollet/keras#getting-started-30-seconds-to-keras) for all the code you need to know).\n",
    "\n",
    "1. Add a dense hidden layer 512 neurons wide that accepts our 784 dimensional input. (What does it mean for a layer to be dense?).\n",
    "2. Give these neurons a 'sigmoid' activation function.\n",
    "3. Add another dense hidden layer 512 nuerons wide, each neuron with a sigmoid activation function.\n",
    "4. Finally, add a dense layer 10 nuerons wide with a 'softmax' activation function. (What do you think this layer is doing?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Your multi-layer perceptron here\n",
    "model.add(Dense(512, input_dim=784))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model requires two other parameters: a loss (objective) function and an optimization method. For our loss function we will use what Keras calls *'categorical_crossentropy'*, but which also goes by 'multiclass logloss'. Keras has a fair amount of objective functions to choose from, but *categorical_crossentropy* is the only one that really makes sense here.\n",
    "\n",
    "We will use stochastic gradient descent ('sgd') for our optimization method. Stochastic, because it randomly selects a sample from our training data to calculate the gradient during backpropogation (or iterates sequentially over a random shuffling of the data). Gradient descent because... err, it's doing gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='sgd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At last, the heavy lifting portion of our code, the fitting of the model.\n",
    "\n",
    "Call model.fit with our training data, a batch_size of 128, and 1 epoch.\n",
    "\n",
    "A *batch* is how many datapoints our neural net uses to calculate the gradient before taking a step in parameter space. Batches are taken sequentially from the entire training set during optimization.\n",
    "\n",
    "An *epoch* is a single pass through the data during optimization. For example, if we are training on 100 data points, and our batch size is 10, then we will have completed an epoch after processing 10 batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "6000/6000 [==============================] - 19s - loss: 1.6659 - acc: 0.4615 - val_loss: 1.1672 - val_acc: 0.6870\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d2945d0>"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you get an I/O error, make sure your model \n",
    "# completely finishes compiling before running this line.\n",
    "model.fit(X_train, Y_train, batch_size=128, nb_epoch=10, validation_data=(X_test, Y_test), show_accuracy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000/5000 [==============================] - 6s     \n",
      "Loss on Test: 2.2071772789\n",
      "Accuracy on Test: 0.3304\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(X_test, Y_test, show_accuracy=True)\n",
    "print(\"Loss on Test:\", results[0])\n",
    "print(\"Accuracy on Test:\", results[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our results are terrible. What happened?!\n",
    "\n",
    "Well, it turns out no one actually uses the sigmoid activation function in practice. The gradient of the logistic function is just too small to be useful, and calculating the exponential in the denominator is too computationally expensive. Instead, replace all your sigmoid activation functions with ReLU ('*relu*') activation functions. The ReLU takes value 0 from -infinity to 0 and value x from 0 to +infinity (equivalent to max(0, x)). Fit your model again and see what happens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "If you've done that, you should have seen a huge improvement. So why is ReLU so much better than sigmoid? They each have their own [pros and cons](http://cs231n.github.io/neural-networks-1/) (see \"Commonly used activation functions\"), but ReLU typically converges 6-8 times faster than sigmoid, since its gradient is always 1 for positive x (sigmoid's gradient is only 1 at x = 0.5). But there's a risk that your neuron will \"die\" if your x value is non-positive too often. The gradient for a non-positive x is zero, so there's no way for the neuron to update its weights once that happens. And if your neuron happens to combine its inputs such that they are non-positive values for every data instance in our training set, the neuron will never change its weights ever again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've only been optimizing our model for a single epoch so far, but in practice neural nets are optimized over tens to thousands of epochs. Go ahead and increase the number of epochs in our call to model.fit to 10 and pass in new arguments *validation_data=(X_test, Y_test)* and *show_accuracy=True*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those results weren't bad, but we can overfit less with *dropout*. Dropout is a form of regularization that woks by \"dropping\" neurons from the network at random at each batch. You can think of this as randomly selecting a proportion of edges in our neural network graph to ignore when doing the feed-forward (i.e., the matrix multiplication) part in the training of our models. Add *Dropout* layers that randomly set 20% of the input units to 0 at each of our hidden layers. You should notice that our validation accuracy increases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's all there is to it! Your neural net architecture should now be identical to the architecture specified in the [original example](https://github.com/fchollet/keras/blob/master/examples/mnist_mlp.py). The only difference is that we train and validate on a subset of the data, since we're using the CPU. If we were on the GPU and training and validating on all 80,000 data points, each epoch would only take 2 seconds and would achieve 98.4% accuracy on the test set after 20 epochs. (Yes, *2 seconds an epoch on 1000% more data*, you read that right!). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's plenty more parameters to adjust, so play around with the batch size, dropout proportion, optimization method, hidden layer width and depth, and different activation functions.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
