{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# treeEnsembles.r\n",
    "# MWL, Lecture 2\n",
    "# Author(s): [Phil Snyder]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An ensemble is a predictor that obtains its predictions by taking a weighted average of multiple \n",
    "single predictors. We will look at three different types of tree-based ensembles: bagging, \n",
    "random forests, and boosting with trees. Both random forests and boosting are methods \n",
    "used in industry every day - and can be very powerful predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "library(\"ElemStatLearn\")\n",
    "data(\"spam\")\n",
    "\n",
    "partition <- sample(nrow(spam), floor(0.7 * nrow(spam)))\n",
    "trainData <- spam[partition,]\n",
    "testData <- spam[-partition,] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagging\n",
    "======"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagging is a portmanteau of 'bootstrap aggregation'. 'Bootstrap' is a sampling method where you \n",
    "sample from a data set *with replacement*. Usually your sample size is equivalent to the size of \n",
    "the original data set. 'Aggregation' refers to the fact that we are aggregating or averaging the \n",
    "predictors (in the case of classification, by having them take a vote on the class) trained on \n",
    "our bootstrap samples. Typically bagging only helps in the case of high variance predictors (such\n",
    "as decision trees) because it reduces variance while maintaining bias (again, see bias-variance \n",
    "trade-off). More info available in [ESL 8.7]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "also installing the dependencies ‘numDeriv’, ‘lava’, ‘prodlim’\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The downloaded source packages are in\n",
      "\t‘/private/var/folders/2b/rjbg45pn0svc9v0rsqlgzyhc0000gn/T/RtmpVIkMOu/downloaded_packages’\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating HTML index of packages in '.Library'\n",
      "Making 'packages.html' ... done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "     FALSE       TRUE \n",
       "0.05286025 0.94713975 "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#install.packages(\"ipred\", repos=\"http://cran.rstudio.com/\")\n",
    "library(\"ipred\")\n",
    "bagg <- bagging(spam ~ ., trainData, nbagg=50) # 50 trees\n",
    "# Of course, normally we would do CV to find the optimal parameters \n",
    "# (implicit here are the rpart parameters)\n",
    "baggPredictions <- predict(bagg, testData)\n",
    "baggResults <- table(baggPredictions == testData$spam) / length(baggPredictions)\n",
    "baggResults"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forests\n",
    "=============="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forests are similar to bagging, except each tree is only trained on a random subset of the \n",
    "dimensions of the original dataset. For example, if our data had dimensions V1, ..., V10\n",
    "we might train the first tree on dimensions V1, V4, V8 and our second tree on V2, V4, V5, etc.\n",
    "We do this so that our individual trees become less *correlated*. The correlation between trees \n",
    "limits how good of a predictor we may obtain by averaging their individual predictions. You \n",
    "may also think of the individual trees in a random forest as being 'experts' in a specific domain. \n",
    "The more limited the domain of the expert, the deeper and more refined the experts knowledge becomes \n",
    "in that domain. For more info see [ESL 15]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "In install.packages(\"randomForest\", repos = \"http://cran.rstudio.com/\"): installation of package ‘randomForest’ had non-zero exit status"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The downloaded source packages are in\n",
      "\t‘/private/var/folders/2b/rjbg45pn0svc9v0rsqlgzyhc0000gn/T/RtmpVIkMOu/downloaded_packages’\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating HTML index of packages in '.Library'\n",
      "Making 'packages.html' ... done\n",
      "randomForest 4.6-12\n",
      "Type rfNews() to see new features/changes/bug fixes.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "   FALSE     TRUE \n",
       "0.044895 0.955105 "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#install.packages(\"randomForest\", repos=\"http://cran.rstudio.com/\")\n",
    "library(\"randomForest\")\n",
    "rf <- randomForest(spam ~ ., trainData, ntree=80) # 80 trees trained on random dimensions\n",
    "# in classification, by default we randomly select m = floor(sqrt(p)) dimensions to train on\n",
    "# where p is the # of dimensions in our original dataset.\n",
    "rfPredictions <- predict(rf, testData)\n",
    "rfResults <- table(rfPredictions == testData$spam) / length(rfPredictions)\n",
    "rfResults"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is our best predictor yet, though random forests have their limits. If the dimensions \n",
    "of our data have very little to do with how the class is determined, then it's likely many of \n",
    "the trees will make decisions based on useless information, adding noise to our predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boosting\n",
    "========="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will do boosting with decision trees, though it is possible to do boosting with different \n",
    "predictors. The general idea behind boosting is that each data point in the training set is given \n",
    "a weight. At first these weights are all equal. We then fit a tree to the train data. For every data \n",
    "point that the tree classifies incorrectly, we increase its weight. We then fit a second tree \n",
    "to the data. The second tree gives the data points with greater weight greater importance, and is \n",
    "less likely to classify them incorrectly. The process then repeats, iterating sequentially through \n",
    "each dimension. This will perhaps become more clear when we talk about loss/objective \n",
    "functions. For more info see [ESL 10]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The downloaded source packages are in\n",
      "\t‘/private/var/folders/2b/rjbg45pn0svc9v0rsqlgzyhc0000gn/T/RtmpVIkMOu/downloaded_packages’\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating HTML index of packages in '.Library'\n",
      "Making 'packages.html' ... done\n",
      "Loading required package: rpart\n",
      "Loading required package: mlbench\n",
      "Loading required package: caret\n",
      "Loading required package: lattice\n",
      "Loading required package: ggplot2\n",
      "\n",
      "Attaching package: ‘adabag’\n",
      "\n",
      "The following object is masked from ‘package:ipred’:\n",
      "\n",
      "    bagging\n",
      "\n",
      "Warning message:\n",
      "In is.na(e1) | is.na(e2): longer object length is not a multiple of shorter object lengthWarning message:\n",
      "In `==.default`(boost$class, testData$spam): longer object length is not a multiple of shorter object length"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "    FALSE      TRUE \n",
       "0.5040373 0.4959627 "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#install.packages(\"adabag\", repos=\"http://cran.rstudio.com/\")\n",
    "library(\"adabag\")\n",
    "boost <- boosting(spam ~ ., trainData, mfinal=100) # 100 boosted trees\n",
    "boostPredictions <- predict(boost, testData)\n",
    "boostResults <- table(boost$class == testData$spam) / length(boost$class)\n",
    "boostResults"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boosting is a powerful idea, and we will generalize it to other predictors later.\n",
    "\n",
    "You may have noticed that it took a fair amount of time to fit our models to the data. This is \n",
    "where the 'computer science' portion of machine learning comes into play. The most powerful \n",
    "predictors (usually ensembles, or various forms of neural nets, or both) can take days to train, \n",
    "even with extreme computational optimization. Next lecture we will take a closer look at the \n",
    "optimization of our model parameters as well as some more archetypical concepts in machine learning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.2.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
